# ğŸ“± Mobile â†’ Laptop LLM Connection

## Use Laptop's Local LLM from Your Mobile

Your mobile can use the Ollama LLM running on your laptop over the network!

---

## ğŸ¯ Why This Matters

**Privacy + Performance:**
- Sensitive queries processed locally (not sent to cloud)
- Use powerful laptop hardware
- No API costs for local queries
- Mobile benefits from laptop's LLM

---

## âš™ï¸ Setup (One Time)

### Step 1: Configure Ollama on Laptop

**Make Ollama accessible on network:**

**Windows:**
```powershell
# Stop Ollama
taskkill /F /IM ollama.exe

# Set environment variable
setx OLLAMA_HOST "0.0.0.0:11434"

# Restart Ollama
ollama serve
```

**macOS/Linux:**
```bash
# Add to ~/.bashrc or ~/.zshrc
export OLLAMA_HOST=0.0.0.0:11434

# Restart terminal and Ollama
ollama serve
```

### Step 2: Get Your Laptop's IP Address

**Windows:**
```powershell
ipconfig
# Look for "IPv4 Address" under your network adapter
# Example: 10.1.10.93
```

**macOS:**
```bash
ifconfig | grep "inet "
# Look for your local network IP
# Example: 10.1.10.93
```

### Step 3: Configure Mobile App

1. **Open app on mobile**: `http://YOUR_LAPTOP_IP:5173`
2. **Login**
3. **Go to**: Settings â†’ LLM Strategy
4. **Select**: ğŸ  Local LLM (Ollama)
5. **In "Ollama Server URL" field**, enter:
   ```
   http://YOUR_LAPTOP_IP:11434
   ```
   Example: `http://10.1.10.93:11434`
6. **Click**: "Save & Test Connection"
7. âœ… Should show: "Connected to Ollama at http://10.1.10.93:11434"

---

## ğŸ”„ How It Works

```
Your Mobile Phone
     â†“
Your Query
     â†“
[WiFi Network]
     â†“
Your Laptop (running Ollama)
     â†“
Local LLM Processing
     â†“
[WiFi Network]
     â†“
Response to Mobile
```

**Benefits:**
- âœ… Processing stays on your network
- âœ… No cloud API needed
- âœ… Private and secure
- âœ… Use laptop's powerful hardware
- âœ… Works on same WiFi network

---

## ğŸ¯ Usage Scenarios

### Scenario 1: At Home

**Setup:**
- Laptop at home running Ollama
- Mobile connected to home WiFi
- Mobile configured with laptop IP

**Usage:**
```
ğŸ“± Mobile â†’ Settings â†’ LLM Strategy â†’ Local LLM
ğŸ¤ Ask question
ğŸ’» Laptop processes with Ollama
ğŸ“± Mobile receives response
```

### Scenario 2: At Office

**Setup:**
- Laptop at office running Ollama
- Mobile on office WiFi
- Same configuration works

**Usage:**
```
ğŸ“± Same as home!
Works on any network where both devices are connected
```

### Scenario 3: On the Go

**When NOT on same network:**
```
ğŸ“± Mobile â†’ Settings â†’ LLM Strategy â†’ Cloud API Only
ğŸ¤ Ask question
â˜ï¸ Uses Claude API
ğŸ“± Mobile receives response
```

**Switch back when home:**
```
ğŸ“± Settings â†’ LLM Strategy â†’ Local LLM
âœ… Back to using laptop's Ollama
```

---

## ğŸ§ª Testing Connection

### Test 1: Can Mobile Reach Laptop?

**From mobile browser:**
```
http://YOUR_LAPTOP_IP:11434
```

**Expected:**
```
Ollama is running
```

**If you see this:** âœ… Network connection works!

**If you don't see this:**
- Check laptop firewall
- Verify both on same WiFi
- Check OLLAMA_HOST is set to 0.0.0.0

### Test 2: Query from Mobile

1. **Open Strategic Advisor on mobile**
2. **Ensure**: Settings â†’ LLM Strategy â†’ Local LLM selected
3. **Ask**: "Hello, are you working?"
4. **Check**: Should get response processed by laptop

**Debug:**
- Check laptop console (where Ollama is running)
- Should see incoming request when you query
- Mobile app console shows which LLM is being used

---

## ğŸ” Security Considerations

### Network Security:

**This is SAFE because:**
- âœ… Only accessible on your private network
- âœ… Not exposed to internet
- âœ… Both devices authenticated
- âœ… All traffic stays local

**Additional Security:**
- Use home/office WiFi only (not public WiFi)
- Don't expose Ollama to internet
- Keep laptop firewall enabled for other ports

---

## âš™ï¸ Configuration Options

### Option 1: Always Use Laptop LLM (Recommended)

**Mobile Settings:**
```
LLM Strategy: Local LLM
Ollama URL: http://LAPTOP_IP:11434
```

**When it works:**
- At home on WiFi
- At office on WiFi
- Anywhere on same network as laptop

**When it doesn't:**
- Outside network (automatically falls back to error)
- Laptop offline

### Option 2: Smart Switching

**At home/office:**
```
LLM Strategy: Local LLM
Ollama URL: http://LAPTOP_IP:11434
```

**When traveling:**
```
LLM Strategy: Cloud API Only
```

**Manually switch based on location!**

---

## ğŸ“ Pro Tips

### Tip 1: Check Connection Before Querying

**In Settings â†’ LLM Strategy:**
- Look for: "âœ… Connected to Ollama at..."
- Green = working
- Yellow = not connected

### Tip 2: Use Fast Models on Laptop

**For mobile use, recommend:**
```
llama3.1:8b (fast, good quality)
phi3:medium (very fast, smaller)
mistral:7b (fast, efficient)
```

**Avoid large models:**
```
llama3.1:70b (too slow for mobile experience)
```

### Tip 3: Keep Laptop Awake

**So mobile can always reach it:**
- Disable laptop sleep when on AC power
- Or wake laptop before using mobile

### Tip 4: Static IP for Laptop (Optional)

**Instead of changing IP each time:**
1. Router settings â†’ DHCP
2. Reserve IP for laptop's MAC address
3. Laptop always gets same IP
4. Mobile config never changes!

---

## ğŸ†˜ Troubleshooting

### "Cannot connect to Ollama"

**Check:**
1. **Laptop Ollama running?**
   ```powershell
   # On laptop
   ollama list
   # Should show models
   ```

2. **Same WiFi?**
   - Both devices on same network
   - Not on guest network

3. **Firewall?**
   ```powershell
   # Windows - Allow port 11434
   netsh advfirewall firewall add rule name="Ollama" dir=in action=allow protocol=TCP localport=11434
   ```

4. **Correct IP?**
   - Run `ipconfig` on laptop
   - Update mobile config if IP changed

### "Slow responses from mobile"

**Normal!** Mobile â†’ Laptop adds network latency:
- Local laptop: ~1-2 seconds
- Mobile â†’ Laptop: ~3-5 seconds

**If too slow:**
- Use smaller/faster model
- Check WiFi signal strength
- Or switch to Cloud API for queries

### "Sometimes works, sometimes doesn't"

**Likely:** Laptop IP changed

**Fix:**
1. Check laptop IP: `ipconfig`
2. Update mobile: Settings â†’ LLM Strategy â†’ Ollama URL
3. Save & Test Connection

**Permanent fix:** Set static IP for laptop

---

## ğŸ“Š Comparison

### Local LLM (via Laptop):

**Pros:**
- âœ… Private (stays on network)
- âœ… No API costs
- âœ… Works offline (on local network)
- âœ… Can use powerful models

**Cons:**
- âŒ Only works on same network
- âŒ Requires laptop running
- âŒ Slightly slower than cloud
- âŒ Need to configure connection

### Cloud API:

**Pros:**
- âœ… Works anywhere
- âœ… Very fast responses
- âœ… Always available
- âœ… No local setup needed

**Cons:**
- âŒ Requires API key
- âŒ Costs per query
- âŒ Data sent to cloud
- âŒ Requires internet

### Recommended: Switch Based on Location

**At Home/Office:**
```
Use: Local LLM (laptop)
Why: Private, free, good quality
```

**Traveling/Outside:**
```
Use: Cloud API
Why: Always works, fast, convenient
```

---

## ğŸ‰ You're Set Up!

Now you can:

âœ… **Use laptop's LLM from mobile** (same network)  
âœ… **Keep sensitive queries private** (local processing)  
âœ… **Save on API costs** (no cloud charges)  
âœ… **Switch easily** between local and cloud  
âœ… **Enjoy powerful laptop hardware** from mobile  

---

## ğŸ“ Quick Reference

**Laptop IP Check:**
```powershell
ipconfig
```

**Mobile Configuration:**
```
Settings â†’ LLM Strategy
Select: Local LLM
URL: http://LAPTOP_IP:11434
Save & Test
```

**Test Connection:**
```
Mobile browser: http://LAPTOP_IP:11434
Should see: "Ollama is running"
```

**Switch to Cloud:**
```
Settings â†’ LLM Strategy
Select: Cloud API Only
```

---

**Your mobile now has access to powerful local LLM processing via your laptop!** ğŸ’»ğŸ“±ğŸ”’
